{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.3"
    },
    "colab": {
      "name": "Assign2_2_logistic_regression_student.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hLcWRu-jp7gM"
      },
      "source": [
        "# MIS 583 Assignment 2-2: Logistic Regression"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oSwr9MgZogRZ"
      },
      "source": [
        "Before we start, please put your name and ID in following format  \n",
        ": LASTNAME Firstname, ?000000000   //   e.g.) 陳耀融, M094020099\n",
        "\n",
        "**Your Answer:**   \n",
        "Hi I'm XXX, XXXXXXXXXX"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "B_MoQiztpxcK"
      },
      "source": [
        "## Logistic Regression\n",
        "\n",
        "Logistic Regression is a member of the linear model family. But not like Linear Regression that outputs a real value, Logistic Regression is used for binary classification.\n",
        "\n",
        "In this tutorial, we will cover:\n",
        "* PyTorch: Tensor operations\n",
        "* Machine Learning: Logistic Regression, gradient descent, preprocessing of data\n",
        "\n",
        "This assignment will walk you through implementing a logistic regression that classifies whether a person is rich or poor using the UCI adult income dataset in PyTorch.\n",
        "\n",
        "The assignment is divied into two parts. \n",
        "* In part 1, you will implement a logistic regression from scratch using PyTorch tensor operations. This will help you gain a better understanding of the theoretical concepts discussed in class.\n",
        "* In part 2, you will use PyTorch nn.Module to build a logsitic regression so that you will get familiar with PyTroch APIs."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "giUId1Naqacs"
      },
      "source": [
        "##  Versions of used packages\n",
        "\n",
        "We will check PyTorch version to make sure everything work properly.\n",
        "\n",
        "We use `python 3.6.9`, `torch==1.6.0`"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Vuw-gNvjqcYe"
      },
      "source": [
        "!python --version\n",
        "!pip freeze | grep torch"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OhdbdJOsrbxL"
      },
      "source": [
        "## Prepare Data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gVbtJxl6rc3t"
      },
      "source": [
        "### Load Data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nPoSgD83teTQ"
      },
      "source": [
        "We use [adult income](https://www.kaggle.com/wenruliu/adult-income-dataset) dataset from UCI machine learning repository.  \n",
        "\n",
        "**Abstrct**  \n",
        "\n",
        "Provide an individual’s education level, age, gender, occupation, and etc, we can predict one's income levels.   \n",
        "\n",
        "**Metadata**  \n",
        "Number of attributes: 14  \n",
        "- income: >50K, <=50K\n",
        "- age: continuous.\n",
        "- workclass: Private, Self-emp-not-inc, Self-emp-inc, Federal-gov, Local-gov, State-gov, Without-pay, Never-worked.\n",
        "- fnlwgt: continuous.\n",
        "- education: Bachelors, Some-college, 11th, HS-grad, Prof-school, Assoc-acdm, Assoc-voc, 9th, 7th-8th, 12th, Masters, 1st-4th, 10th, Doctorate, 5th-6th, Preschool.\n",
        "- education-num: continuous.\n",
        "- marital-status: Married-civ-spouse, Divorced, Never-married, Separated, Widowed, Married-spouse-absent, Married-AF-spouse.\n",
        "- occupation: Tech-support, Craft-repair, Other-service, Sales, Exec-managerial, Prof-specialty, Handlers-cleaners, Machine-op-inspct, Adm-clerical, Farming-fishing, Transport-moving, Priv-house-serv, Protective-serv, Armed-Forces.\n",
        "- relationship: Wife, Own-child, Husband, Not-in-family, Other-relative, Unmarried.\n",
        "- race: White, Asian-Pac-Islander, Amer-Indian-Eskimo, Other, Black.\n",
        "- sex: Female, Male.\n",
        "- capital-gain: continuous.\n",
        "- capital-loss: continuous.\n",
        "- hours-per-week: continuous.\n",
        "- native-country: United-States, Cambodia, England, Puerto-Rico, Canada, Germany, Outlying-US(Guam-USVI-etc), India, Japan, Greece, South, China, Cuba, Iran, Honduras, Philippines, Italy, Poland, Jamaica, Vietnam, Mexico, Portugal, Ireland, France, Dominican-Republic, Laos, Ecuador, Taiwan, Haiti, Columbia, Hungary, Guatemala, Nicaragua, Scotland, Thailand, Yugoslavia, El-Salvador, Trinadad&Tobago, Peru, Hong, Holand-Netherlands."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FhKEp8b1r3V-"
      },
      "source": [
        "# Download it from cu and upload to colab\n",
        "\n",
        "# or from my Dropbox\n",
        "!wget -q -N https://www.dropbox.com/s/1jqeipgof7tukln/data.zip\n",
        "!unzip -n data.zip"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cqO8DiB6VRQZ"
      },
      "source": [
        "It contain 4 csv files: X_train, X_test, Y_train and Y_test, which represent training data, training labels, test data and test labels, respectively.\n",
        "\n",
        "Each row in X has 106 fields  \n",
        "Each row Y represents the true label of 0 (poor) and 1 (rich).  \n",
        "\n",
        "Or you can load train.csv raw data and do your pre-processing."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TRq8vX4kn24u"
      },
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "# Pre-Processed Version\n",
        "X_train_raw = pd.read_csv('data/X_train.csv')\n",
        "Y_train_raw = pd.read_csv('data/Y_train.csv')\n",
        "X_test_raw = pd.read_csv('data/X_test.csv')\n",
        "Y_test_raw = pd.read_csv('data/Y_test.csv')\n",
        "\n",
        "X_train_raw.head()\n",
        "\n",
        "# Or you can do preprocess by yourself\n",
        "# df = pd.read_csv('data/train.csv')\n",
        "# ...some preprocessing\n",
        "# ...remember split into 4 variables with same name by yourself"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7XvstELNg7wO"
      },
      "source": [
        "You already notice that there are two datasets.   One is called \"training set\", and another is called \"testing set\".  \n",
        "Training set is like **homework of model**. Model will do it serval times and get correct answer to improve itself.  \n",
        "Testing set is like **quiz of model**. Testing set only provide x and evaluate how good your model is.  \n",
        "Train-Test-Split could prevent our model from overfitting by check performence between two datasets.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ckRcWuGlZLYV"
      },
      "source": [
        "### Pre-Processing Data (10 points)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TGlPZ1Okx1Z0"
      },
      "source": [
        "In a data science process, data cleansing usually cost the most of time.  \n",
        "Dirty data will cause overfitting or make us ignore those important feature. Even if you don't have correct cleansing and format, your model can't run anymore."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WuuY4McyXrb3"
      },
      "source": [
        "import torch\n",
        "################################################################################\n",
        "# TODO: Implement Standard Deviation Normalization mentioned in class.         #\n",
        "# train_data has shape(n_train, feature_dim)                                   #\n",
        "# test_data has shape(n_test, feature_dim)                                     #\n",
        "# hint: You should count mean and std in training, and apply it to test.       #\n",
        "################################################################################\n",
        "def compute_mu_std(data):\n",
        "    '''\n",
        "    Arguments:\n",
        "    data -- torch.Tesor, contain data with shape(n, feature_dim)\n",
        "\n",
        "    Return:\n",
        "    mu - torch.Tensor, mean value of columns of data with shape(feature_dim)\n",
        "    sigma - torch.Tensor, std value of columns of data with shape(feature_dim)\n",
        "    '''\n",
        "    mu = ...\n",
        "    sigma = ...\n",
        "    return mu, sigma\n",
        "\n",
        "def standard_normalize(data, mu, std):\n",
        "    '''\n",
        "    Arguments:\n",
        "    data -- torch.Tesor, contain data with shape(n, feature_dim)\n",
        "    mu - torch.Tensor, mean value of columns of data with shape(feature_dim)\n",
        "    sigma - torch.Tensor, std value of columns of data with shape(feature_dim)\n",
        "\n",
        "    Return:\n",
        "    data -- torch.Tensor, normalized data with given mu and sigma\n",
        "    '''\n",
        "    data = ...\n",
        "    return data\n",
        "\n",
        "# Better not to change code below, except you wanna do your pre-processing\n",
        "# pandas > (.values) > numpy > (tensor) > torch.tensor\n",
        "X_train = torch.tensor(X_train_raw.values, dtype=torch.float)\n",
        "X_test = torch.tensor(X_test_raw.values, dtype=torch.float)\n",
        "\n",
        "mu, sigma = compute_mu_std(X_train)\n",
        "\n",
        "# output the first 5 values of mu and sigma\n",
        "print('the first 5 means are:', mu[:5])\n",
        "print('the first 5 sigma are:', sigma[:5])\n",
        "\n",
        "f_dim = X_train.shape[1]\n",
        "assert mu.shape == torch.Size([f_dim]), 'Shape of mu is incorrect.'\n",
        "assert sigma.shape == torch.Size([f_dim]), 'Shape of sigma is incorrect.'\n",
        "\n",
        "X_train = standard_normalize(X_train, mu, sigma)\n",
        "X_test = standard_normalize(X_test, mu, sigma)\n",
        "################################################################################\n",
        "#                             END OF YOUR CODE                                 #\n",
        "################################################################################\n",
        "# make shape(n, 1) > shape(n), like [[1, 2, 3]] > [1, 2, 3]\n",
        "Y_train = torch.tensor(Y_train_raw.values).squeeze()\n",
        "Y_test = torch.tensor(Y_test_raw.values).squeeze()\n",
        "\n",
        "print('Shape of X_train:', X_train.shape) \n",
        "print('Shape of X_test:', X_test.shape)  \n",
        "print('Shape of y_train:', Y_train.shape)\n",
        "print('Shape of y_test:', Y_test.shape)\n",
        "\n",
        "\n",
        "assert X_train.dim() == 2\n",
        "assert Y_train.dim() == 1\n",
        "# You can ONLY change assertion are there.\n",
        "# IF you are writting YOUR pre-processing, cuz you will have different shape.\n",
        "assert X_train.shape == (32561, 106)\n",
        "assert X_test.shape == (16281, 106)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "U5jtBkbJyBym"
      },
      "source": [
        "Beacause the size of dataset used in Deep Learning and Computer Vision field usually be ginormous.  You can not load you model and whole data into memory(GPU or RAM) in the same time.  \n",
        "Mini-Batch SGD method mentioned in class come in handy. We will split dataset into batches."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zX1pqlGnn25M"
      },
      "source": [
        "def make_batch(data, batch_size=128, drop_last=True):\n",
        "    '''\n",
        "    Split dataset into batches.\n",
        "\n",
        "    Arguments:\n",
        "    data -- torch.Tensor, data with shape(n, ...)\n",
        "    batch_size -- int, how many data in your batch\n",
        "    drop_last -- boolean, drop last datas if your remaining data is < batch_size\n",
        "\n",
        "    Return:\n",
        "    out -- torch.Tensor(dtype=torch.float) with shape(n_batch, batch_size, ...)\n",
        "    '''\n",
        "    if drop_last:\n",
        "        n = data.shape[0] // batch_size\n",
        "    else:\n",
        "        n = ((data.shape[0] - 1) // batch_size) + 1\n",
        "    out = np.empty(torch.Size((n, batch_size)) + data.shape[1:], dtype=np.float32)\n",
        "    print(out.shape)\n",
        "    for b in range(len(out)):\n",
        "        out[b] = data[b * batch_size:(b + 1) * batch_size]\n",
        "    return torch.tensor(out)\n",
        "\n",
        "X_train_batch = make_batch(X_train)\n",
        "Y_train_batch = make_batch(Y_train)\n",
        "X_test_batch = make_batch(X_test)\n",
        "Y_test_batch = make_batch(Y_test)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "U4atwzT3aPi3"
      },
      "source": [
        "Finally! We have made all data prepared.  \n",
        "Let's go develop our model."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "87KYcWknS95z"
      },
      "source": [
        "# Part 1: Implement Logistic Regression from Scratch"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Y_moLy20cEn_"
      },
      "source": [
        "## Utility function"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OH_4NKB9dsZ3"
      },
      "source": [
        "### Activation Function (5 points)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ifQD-YnvcY3k"
      },
      "source": [
        "The biggest difference between Logistic Regression and Linear Regression is activation function.  \n",
        "Logistic Regression use logistic function(or called sigmoid function)  \n",
        "It takes any real input t and outputs a value between zero and one.  \n",
        "\n",
        "**Slide: ch03 p85**\n",
        "\n",
        "![](https://upload.wikimedia.org/wikipedia/commons/thumb/8/88/Logistic-curve.svg/640px-Logistic-curve.svg.png)\n",
        "\n",
        "*Source: wikipedia*"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CuzoweBRn25o"
      },
      "source": [
        "def sigmoid(z): \n",
        "    '''\n",
        "    Compute the sigmoid of z\n",
        "    Ref: Slide ch03 p85\n",
        "\n",
        "    Arguments:\n",
        "    z -- A scalar or torch.Tensor of any size.\n",
        "\n",
        "    Return:\n",
        "    s -- torch.Tensor, sigmoid(z)\n",
        "    '''\n",
        "    if not isinstance(z, torch.Tensor):\n",
        "        z = torch.tensor(z, dtype=torch.float)\n",
        "    ############################################################################\n",
        "    # TODO: Implement sigmoid(or called logistic) function                     #\n",
        "    # Slide: ch03 p85                                                          #\n",
        "    ############################################################################\n",
        "    pass\n",
        "    ############################################################################\n",
        "    #                             END OF YOUR CODE                             #\n",
        "    ############################################################################\n",
        "    # prevent out is Inf or -Inf\n",
        "    out = torch.clamp(out, 1e-6, 1-1e-6)\n",
        "    return out\n",
        "\n",
        "s_t1 = sigmoid(0)\n",
        "s_t2 = sigmoid(2)\n",
        "s_t3 = sigmoid(-1)\n",
        "print('sigmoid(0)  =', s_t1)\n",
        "print('sigmoid(2)  =', s_t2)\n",
        "print('sigmoid(-1) =', s_t3)\n",
        "msg = 'Your sigmoid: {} isn\\'t correct'\n",
        "assert (s_t1 - 0.5) < 0.001, msg.format(sigmoid(0))\n",
        "assert (s_t2 - 0.8808) < 0.001, msg.format(sigmoid(2))\n",
        "assert (s_t3 - 0.2689) < 0.001, msg.format(sigmoid(-1))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LVnUmO0DdwjF"
      },
      "source": [
        "### Loss Function (10 points)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "inIo1GtRdylu"
      },
      "source": [
        "Cross Entropy is very important in information theory.  \n",
        "It calculate the difference between two probability distributions.  \n",
        "**Slide: ch03 p78**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sboq4fXhn250"
      },
      "source": [
        "def binary_cross_entropy(y_true, y_pred):\n",
        "    '''\n",
        "    Compute the binary cross entropy of inputs.\n",
        "    Ref: Slide ch03 p88\n",
        "\n",
        "    Arguments:\n",
        "    y_true -- torch.Tensor, True data with shape(n_size).\n",
        "    y_pred -- torch.Tensor, Predicted data with shape(n_size)\n",
        "\n",
        "    Return:\n",
        "    s -- torch.Tensor, binary_cross_entropy(z)\n",
        "    '''\n",
        "    ############################################################################\n",
        "    # TODO: Implement binary cross entropy function                            #\n",
        "    ############################################################################\n",
        "    loss = ...\n",
        "    ############################################################################\n",
        "    #                             END OF YOUR CODE                             #\n",
        "    ############################################################################\n",
        "    return loss\n",
        "\n",
        "b_t1 = binary_cross_entropy(torch.tensor([1]), torch.tensor([0.5]))\n",
        "b_t2 = binary_cross_entropy(torch.tensor([1]), torch.tensor([0.8]))\n",
        "b_t3 = binary_cross_entropy(torch.tensor([1]), torch.tensor([0.9]))\n",
        "print('bce(1, 0.5)', b_t1)\n",
        "print('bce(1, 0.8)', b_t2)\n",
        "print('bce(1, 0.9)', b_t3)\n",
        "msg = 'Your bce: {} isn\\'t correct'\n",
        "assert (b_t1 - 0.6931) < 0.001, msg.format(b_t1)\n",
        "assert (b_t2 - 0.2231) < 0.001, msg.format(b_t1)\n",
        "assert (b_t3 - 0.1054) < 0.001, msg.format(b_t1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0OswC-h-e1-0"
      },
      "source": [
        "## Logistic Regression Model (50 points)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uu-Ls8DefPGK"
      },
      "source": [
        "You are almost there!  \n",
        "Next step(also the last step), we will combine all of them together.  \n",
        "Get our model work, count the loss, get the gradinet and using gradient-descent method to update our model.  \n",
        "**Slide: ch03 p111**\n",
        " \n",
        "Good luck!\n",
        "```python\n",
        "num_epochs = ... # specify the number of epochs to train initialize parameters w, b\n",
        "for epoch in range(num_epochs):\n",
        "    shuffle training data\n",
        "    for each batch:\n",
        "        forward propagation to get the predictions/outputs \n",
        "        compute loss\n",
        "        backward propagation to get the gradients\n",
        "        update parameters using their gradients\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cglTIH1NrdIO"
      },
      "source": [
        "You should fill out each methods(init, forward, backward, optimizer...)  \n",
        "Inputs, outputs, format and description is written in docstring `'''doc string there'''`  \n",
        "**You CANNOT remove any assertion.**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4LOzsSk7n26G"
      },
      "source": [
        "class LogisticRegression:\n",
        "    def __init__(self, lr, feature_dim):\n",
        "        '''\n",
        "        Initiate learn_rate\n",
        "        Initiate weights and bias to zeros with correct shape.\n",
        "        w and b should be torch.tensor(...)\n",
        "        The shape of w should be (feature_dim)\n",
        "        The shape of b should be (1)\n",
        "\n",
        "        Arguments:\n",
        "        lr -- float, learn_rate to update weight (0 < lr <= 1).\n",
        "        feature_dim -- int, How many features your data have.\n",
        "        '''\n",
        "        ########################################################################\n",
        "        #                           Your code there                            #\n",
        "        ########################################################################\n",
        "        self.w = ...\n",
        "        self.b = ...\n",
        "        self.lr = ...\n",
        "        ########################################################################\n",
        "        #                           End of your code                           #\n",
        "        ########################################################################\n",
        "        assert self.w.shape == torch.Size([feature_dim]), 'shape of w is incorrect'\n",
        "        assert self.b.dtype == torch.float, 'dtype of b should be float'\n",
        "\n",
        "        # initialize dw, db to zeros\n",
        "        self.grads = {'dw': torch.zeros(feature_dim, dtype=torch.float), \n",
        "                      'db': torch.tensor(0, dtype=torch.float)}\n",
        "\n",
        "    def forward(self, x):\n",
        "        '''\n",
        "        Compute Logistic Regression using our w and b for input x.\n",
        "        Store it in self.out, backward need it.\n",
        "\n",
        "        Arguments:\n",
        "        x -- torch.Tensor, input data with shape(n, feature_dim).\n",
        "\n",
        "        Return:\n",
        "        out -- sigmoid(x * w + b)\n",
        "        '''\n",
        "        ########################################################################\n",
        "        #                           Your code there                            #\n",
        "        ########################################################################\n",
        "        out = ...\n",
        "        ########################################################################\n",
        "        #                           End of your code                           #\n",
        "        ########################################################################\n",
        "        self.out = out\n",
        "        assert (out > 0).all() and (out < 1).all(), 'Output should > 0 and < 1'\n",
        "        assert out.shape == torch.Size([x.shape[0]]), 'Shape of output is incorrect.'\n",
        "        return self.out\n",
        "    \n",
        "    def backward(self, x, y_true):\n",
        "        '''\n",
        "        After forward, calculate gradient of w and b(dw and db).\n",
        "        Store dw and db in self.grads.\n",
        "        Ref: Slide ch03 107\n",
        "\n",
        "        Arguments:\n",
        "        x -- torch.Tensor, input data with shape(n,).\n",
        "        y_true -- torch.Tensor, true data with shape(n,).\n",
        "        '''\n",
        "        y_pred = self.out\n",
        "        ########################################################################\n",
        "        #                           Your code there                            #\n",
        "        ########################################################################\n",
        "        dw = ...\n",
        "        db = ...\n",
        "        ########################################################################\n",
        "        #                           End of your code                           #\n",
        "        ########################################################################\n",
        "\n",
        "        assert dw.shape == self.w.shape, 'Shape of dw is incorrect.'\n",
        "        assert db.shape == self.b.shape, 'Shape of db is incorrect.'\n",
        "\n",
        "        self.grads = {'dw': dw, 'db': db}\n",
        "\n",
        "    def optimize(self):\n",
        "        '''\n",
        "        Implement SGD algorithm.\n",
        "        Use gradient and lr update weight and bias.\n",
        "        Remember store back into class.\n",
        "        Ref: Slide ch03 p113\n",
        "        '''\n",
        "        ########################################################################\n",
        "        #                           Your code there                            #\n",
        "        ########################################################################\n",
        "        pass\n",
        "        ########################################################################\n",
        "        #                           End of your code                           #\n",
        "        ########################################################################\n",
        "\n",
        "    def predict(self, x):\n",
        "        '''\n",
        "        Compute output of our model like forward method.\n",
        "        But append a threshold x > 0.5 to output label.\n",
        "        hint: you can re-use forward method\n",
        "\n",
        "        Arguments:\n",
        "        x -- torch.Tensor, input data with shape(n, feature_dim).\n",
        "\n",
        "        Return:\n",
        "        out -- sigmoid(x * w + b) > 0.5 with shape(n,)\n",
        "        '''\n",
        "        ########################################################################\n",
        "        #                           Your code there                            #\n",
        "        ########################################################################\n",
        "        out = ...\n",
        "        ########################################################################\n",
        "        #                           End of your code                           #\n",
        "        ########################################################################\n",
        "        assert out.shape == torch.Size([x.shape[0]]), 'Shape of output is incorrect'\n",
        "        return out"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1RMn82737BAW"
      },
      "source": [
        "################################################################################\n",
        "#                             Hyper Parameters                                 #\n",
        "# You can modify these parameter to achieve higher accuracy                    #\n",
        "################################################################################\n",
        "lr = 3e-2\n",
        "max_epochs = 50\n",
        "log_interval = 5\n",
        "\n",
        "model = LogisticRegression(lr=lr, feature_dim=X_train.shape[1])\n",
        "\n",
        "train_loss_list = []\n",
        "train_acc_list = []\n",
        "test_loss_list = []\n",
        "test_acc_list = []"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KUzf_a3-aa6F"
      },
      "source": [
        "### Put everything together (5 points)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QXzuzR8Xn26N"
      },
      "source": [
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "for epoch in range(1, max_epochs + 1):\n",
        "    idxs = torch.randperm(X_train_batch.shape[0]) # make rand idx\n",
        "    data_loader = ((X_train_batch[i], Y_train_batch[i]) for i in idxs)\n",
        "\n",
        "    loss_list = [] # save train_loss\n",
        "    for i, (x, y) in enumerate(data_loader):\n",
        "        ########################################################################\n",
        "        # TODO: Combine all the functions together                             #\n",
        "        ########################################################################\n",
        "        # hints:\n",
        "        # 1. get output from forward method, call forward()\n",
        "        # 2. calculate loss using binary_cross_entropy()\n",
        "        # 3. using backward method to calculate dw, db, call backward()\n",
        "        # 4. apply SGD optimize to update model's weights, call optimize()\n",
        "        pass\n",
        "        ########################################################################\n",
        "        #                         END OF YOUR CODE                             #\n",
        "        ########################################################################\n",
        "\n",
        "        loss_list.append(loss)\n",
        "\n",
        "    train_loss_list.append(sum(loss_list) / len(loss_list))\n",
        "    train_acc_list.append(accuracy_score(Y_train, model.predict(X_train.float())))\n",
        "    test_loss_list.append(binary_cross_entropy(Y_test, model.forward(X_test.float())))\n",
        "    test_acc_list.append(accuracy_score(Y_test, model.predict(X_test.float())))\n",
        "    if epoch % log_interval == 0:\n",
        "        print('=' * 20, 'Epoch', epoch, '=' * 20)\n",
        "        print('Train loss:', train_loss_list[-1], 'acc:', train_acc_list[-1])\n",
        "        print('Test loss: ', test_loss_list[-1], 'acc:', test_acc_list[-1])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZrC0_PiyMgfW"
      },
      "source": [
        "# plot loss and acc\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "plt.figure(figsize=(12, 4))\n",
        "plt.plot(range(len(train_loss_list)), train_loss_list)\n",
        "plt.plot(range(len(test_loss_list)), test_loss_list, c='r')\n",
        "plt.legend(['train', 'test'])\n",
        "plt.title('Loss')\n",
        "plt.show()\n",
        "plt.figure(figsize=(12, 4))\n",
        "plt.plot(range(len(train_acc_list)), train_acc_list)\n",
        "plt.plot(range(len(test_acc_list)), test_acc_list, c='r')\n",
        "plt.legend(['train', 'test'])\n",
        "plt.title('Acc')\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kgIWwcZIn263"
      },
      "source": [
        "# Implement Logsitic Regression Using PyTorch nn.Module APIs"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-2fKjMvVkPWo"
      },
      "source": [
        "Congratulation! It is all the details of Logistic Regression.  \n",
        "Now, I will show you how to implement it by PyTorch in few lines.    \n",
        "You would be amazed by this powerful and beautiful tool, maybe."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nuzoiHyan264"
      },
      "source": [
        "import torch\n",
        "from torch import nn\n",
        "import torch.nn.functional as F"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "s6SNWep2zNmF"
      },
      "source": [
        "Let's make some dataset with PyTorch.  \n",
        "You can notice there are dataset and data_loader two classes.  \n",
        "DataLoader provide batch, shuffle and many more features about Dataset.  \n",
        "We will dicuss more topics and situations."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Vlv-ziS2g94b"
      },
      "source": [
        "from torch.utils.data import TensorDataset, DataLoader\n",
        "\n",
        "train_dataset = TensorDataset(X_train, Y_train.unsqueeze(dim=1).float())\n",
        "test_dataset = TensorDataset(X_test, Y_test.unsqueeze(dim=1).float())\n",
        "train_data = DataLoader(train_dataset, batch_size=128, shuffle=True)\n",
        "test_data = DataLoader(test_dataset, batch_size=128, shuffle=False)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5COtAwHQ0p3a"
      },
      "source": [
        "PyTorch provide very convienece way to construct your model.  \n",
        "1. all model should inherit `nn.Module`.  \n",
        "2. in `__init__` method, remember call `super().__init__()` and define your model layers.  \n",
        "3.  in `forward()` method connect all your layers to do computation.\n",
        "\n",
        "PyTorch will using [autograd mechanic](https://pytorch.org/tutorials/beginner/blitz/autograd_tutorial.html) to generate `backward()` and get gradient **automatically**.  \n",
        "We get the same model with less effort."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BnGYWc25blAF"
      },
      "source": [
        "### Define a logistic regression using nn.Module (10 points)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sk5puwUJn27F"
      },
      "source": [
        "class TorchLogisticRegression(nn.Module):\n",
        "    def __init__(self, feature_dim):\n",
        "        '''\n",
        "        Initiate weights and bias of our model.\n",
        "\n",
        "        Arguments:\n",
        "        feature_dim -- int, How many features your data have.\n",
        "        '''\n",
        "        super().__init__()\n",
        "        # init the weight AND bias by nn.Linear\n",
        "        ########################################################################\n",
        "        # TODO: use nn.xxx method to generate a linear model part              #\n",
        "        #   you can define one layer with wieght and bias                      #\n",
        "        ########################################################################\n",
        "        self.linear = ...\n",
        "        ########################################################################\n",
        "        #                           End of your code                           #\n",
        "        ########################################################################\n",
        "    \n",
        "    def forward(self, x):\n",
        "        if not isinstance(x, torch.Tensor):\n",
        "            x = torch.Tensor(x)\n",
        "        ########################################################################\n",
        "        # TODO: forward your model and get output                              #\n",
        "        #   Don't forget activation function. But, you can't use previous      #\n",
        "        #   defined sigmoid, try to search in PyTorch docs                     #\n",
        "        ########################################################################\n",
        "        out = ...\n",
        "        assert out.shape == torch.Size([x.shape[0], 1]), 'Shape of output is incorrect'\n",
        "        ########################################################################\n",
        "        #                           End of your code                           #\n",
        "        ########################################################################\n",
        "        return out"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "83imqLRQ0v7M"
      },
      "source": [
        "We have made our model!  \n",
        "Next, PyTorch also provide many utility function(loss, optmizer...etc).  \n",
        "You can define them in one-line."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FtOkPO6Ga0Fw"
      },
      "source": [
        "### Define loss and optimizer (5 points)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4yIRtfYan27M"
      },
      "source": [
        "# 106 features in our income dataset\n",
        "torch_model = TorchLogisticRegression(X_train.shape[1])\n",
        " \n",
        "########################################################################\n",
        "# TODO: Define loss and optmizer functions                             #\n",
        "#   please use Binary Cross Entropy and SGD optimizer                  #\n",
        "# hint: torch.nn and torch.optim                                       #\n",
        "########################################################################\n",
        "criterion = ...\n",
        "params = ... # call a method of model to get parameters\n",
        "optimizer = ... # throw param into optimizer some_optimier(param, lr=...)\n",
        "########################################################################\n",
        "#                           End of your code                           #\n",
        "########################################################################"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zle9KuFcbwMP"
      },
      "source": [
        "### Train the model (5 points)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bZFxE7Y9iLfl"
      },
      "source": [
        "Let's define train function.  \n",
        "It will iterate inputed data 1 epoch and update model with optmizer.  \n",
        "Finally, calculate mean loss and total accuracy."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VM93brDshO6E"
      },
      "source": [
        "def train(data, model, criterion, optimizer):\n",
        "    '''\n",
        "    Argement:\n",
        "    data -- iterable data, typr torch.utils.data.Dataloader is prefer\n",
        "    model -- nn.Module, model contain forward to predict output\n",
        "    criterion -- loss function, used to evaluate goodness of model\n",
        "    optimizer -- optmizer function, method for weight updating\n",
        "    '''\n",
        "    model.train()\n",
        "    \n",
        "    loss_list = []\n",
        "    total_count = 0\n",
        "    acc_count = 0\n",
        "    for x, y in data:\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        ########################################################################\n",
        "        # Training part with data, model, criterion and optimizer              #\n",
        "        ########################################################################\n",
        "        # Like training part we implement above, but PyTorch version\n",
        "        # 1. get output from model, use model()\n",
        "        # 2. calculate loss using criterion(y_pred, y_true)\n",
        "        # 3. backward method of loss to calculate gradient\n",
        "        # 4. call optimizer to update model's weights\n",
        "        ########################################################################\n",
        "        #                           End of your code                           #\n",
        "        ########################################################################\n",
        "\n",
        "        total_count += out.shape[0]\n",
        "        acc_count += ((out > 0.5) == y).sum().item()\n",
        "        loss_list.append(loss.item())\n",
        "    acc = acc_count / total_count\n",
        "    loss = sum(loss_list) / len(loss_list)\n",
        "    return acc, loss"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KmDy1GTq_H2a"
      },
      "source": [
        "Next part is test function.  \n",
        "It works as training function without optmizer and weigght-updating part."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "USzbBgGEoTRu"
      },
      "source": [
        "def test(data, model, criterion):\n",
        "    model.eval()\n",
        "    \n",
        "    loss_list = []\n",
        "    total_count = 0\n",
        "    acc_count = 0\n",
        "    with torch.no_grad():\n",
        "        for x, y in data:\n",
        "            ####################################################################\n",
        "            # Testing part with data, model and criterion                      #\n",
        "            ####################################################################\n",
        "            # Like training part without weight updating\n",
        "            # 1. get output from model, use model()\n",
        "            # 2. calculate loss using criterion(y_pred, y_true)\n",
        "            ####################################################################\n",
        "            #                           End of your code                       #\n",
        "            ####################################################################\n",
        "\n",
        "            total_count += out.shape[0]\n",
        "            acc_count += ((out > 0.5) == y).sum().item()\n",
        "            loss_list.append(loss.item())\n",
        "\n",
        "    acc = acc_count / total_count\n",
        "    loss = sum(loss_list) / len(loss_list)\n",
        "    return acc, loss"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "knXu74jCiuxP"
      },
      "source": [
        "There is truly final step!  \n",
        "Call train and test function in a loop.  \n",
        "Take a break and wait."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rcVulKkFJRtI"
      },
      "source": [
        "# Hyper Parameters\n",
        "max_epochs = 50\n",
        "log_interval = 5\n",
        "\n",
        "train_acc_list = []\n",
        "train_loss_list = []\n",
        "test_acc_list = []\n",
        "test_loss_list = []\n",
        "\n",
        "for epoch in range(1, max_epochs + 1):\n",
        "    train_acc, train_loss = train(train_data, torch_model, criterion, optimizer)\n",
        "    test_acc, test_loss = test(test_data, torch_model, criterion)\n",
        "\n",
        "    train_acc_list.append(train_acc)\n",
        "    train_loss_list.append(train_loss)\n",
        "    test_acc_list.append(test_acc)\n",
        "    test_loss_list.append(test_loss)\n",
        "    if epoch % log_interval == 0:\n",
        "        print('=' * 20, 'Epoch', epoch, '=' * 20)\n",
        "        print('Train Acc: {:.6f} Train Loss: {:.6f}'.format(train_acc, train_loss))\n",
        "        print('Test Acc: {:.6f} Test Loss: {:.6f}'.format(test_acc, test_loss))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4ifzgfp7iq2m"
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "plt.figure(figsize=(12, 4))\n",
        "plt.plot(range(len(train_loss_list)), train_loss_list)\n",
        "plt.plot(range(len(test_loss_list)), test_loss_list, c='r')\n",
        "plt.legend(['train', 'test'])\n",
        "plt.title('Loss')\n",
        "plt.show()\n",
        "plt.figure(figsize=(12, 4))\n",
        "plt.plot(range(len(train_acc_list)), train_acc_list)\n",
        "plt.plot(range(len(test_acc_list)), test_acc_list, c='r')\n",
        "plt.legend(['train', 'test'])\n",
        "plt.title('Acc')\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_wX87wzXhmYg"
      },
      "source": [
        "# Comparison of our model and torch's model\n",
        "print('-' * 20, 'weight' , '-' * 20)\n",
        "print('Ours', model.w[:20])\n",
        "print('PyTorch', torch_model.linear.weight[0][:20])\n",
        "print('-' * 20, 'bias' , '-' * 20)\n",
        "print('Ours', model.b)\n",
        "print('PyTorch', torch_model.linear.bias)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4XTB6EF6mH2Q"
      },
      "source": [
        "Their weights and bias almost be the same.  \n",
        "We got similar results, but with less effort!  \n",
        "Let's meet PyTorch and be its friend in the next few weeks!  "
      ]
    }
  ]
}