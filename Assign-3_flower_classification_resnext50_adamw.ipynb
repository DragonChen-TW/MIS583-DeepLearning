{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hLcWRu-jp7gM"
   },
   "source": [
    "# MIS 583 Assignment 3: Flower Classfication"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "oSwr9MgZogRZ"
   },
   "source": [
    "Before we start, please put your name and ID in following format: <br>\n",
    "NAME, ?000000000   e.g. 陳琨翔, M094020003\n",
    "\n",
    "**Your Answer:**   \n",
    "Hi I'm 陳耀融, M094020055"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "B_MoQiztpxcK"
   },
   "source": [
    "## Flower Classification\n",
    "\n",
    "Image classification is a core and fundamental task in computer vision.\n",
    "\n",
    "In the assignment, you will implement a multi-class image classifier to recognize flowers.\n",
    "\n",
    "You will design and train a deep convolutional network from scratch to predict the class label of a flower image. This will help you gain experience with network design and get more familiar with PyTorch.\n",
    "\n",
    "**Please note that you’re not allowed to use a pre-trained model.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "b1NQv3Ey3cVS"
   },
   "source": [
    "## Kaggle Competition\n",
    "Kaggle is an online community of data scientists and machine learning practitioners. Kaggle allows users to find and publish data sets, explore and build models in a web-based data-science environment, work with other data scientists and machine learning engineers, and enter competitions to solve data science challenges.\n",
    "\n",
    "This assignment use kaggle to calculate your grade.  \n",
    "Please use this [**LINK**](https://www.kaggle.com/t/a16786b7da97419f9ba90b495dab08aa) to join the competition."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "giUId1Naqacs"
   },
   "source": [
    "##  Versions of used packages\n",
    "\n",
    "We will check PyTorch version to make sure everything work properly.\n",
    "\n",
    "We use `python 3.6.9`, `torch==1.6.0`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 110
    },
    "id": "Vuw-gNvjqcYe",
    "outputId": "346b960c-fc65-4294-c7bb-5c87e96070b4"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Python 3.6.8\n",
      "torch==1.6.0\n",
      "torchsummary==1.5.1\n",
      "torchvision==0.7.0\n"
     ]
    }
   ],
   "source": [
    "# in our lab's server\n",
    "# python=3.6.8, torch=1.6.0\n",
    "!python3 --version\n",
    "!pip3 freeze | grep torch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_FydjP4MTWuL"
   },
   "source": [
    "## Error handling\n",
    "\n",
    "**RuntimeError: CUDA out of memory...**\n",
    "> 發生原因可能為讀取的 batch 過大或是記憶體未釋放乾淨。若縮小 batch size 後仍出現錯誤請按照以下步驟重新載入 colab。\n",
    "\n",
    "1. Click 「Runtime」\n",
    "2. Click 「Factor reset runtime」\n",
    "3. Click 「Reconnect」\n",
    "4. Reload all chunk\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "OhdbdJOsrbxL"
   },
   "source": [
    "# Prepare Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nPoSgD83teTQ"
   },
   "source": [
    "We use [Flowers Recognition](https://www.kaggle.com/alxmamaev/flowers-recognition) dataset.\n",
    "This is collected by Alexander Mamaev.\n",
    "\n",
    "**Abstrct**  \n",
    "\n",
    "This dataset contains 4323 images of flowers.\n",
    "The data collection is based on the data flicr, google images, yandex images.\n",
    "You can use this datastet to recognize plants from the photo.\n",
    "\n",
    "The pictures are divided into five classes: \n",
    "+ daisy\n",
    "+ tulip\n",
    "+ rose\n",
    "+ sunflower\n",
    "+ dandelion\n",
    "\n",
    "For each class there are about 800 photos. Photos are not high resolution, about 320x240 pixels. Photos are not reduced to a single size, they have different proportions!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "IiMThsYeDa2O"
   },
   "source": [
    "## Get Data\n",
    "\n",
    "請先到共用雲端硬碟將檔案`flower_data.zip`，建立捷徑到自己的雲端硬碟中。\n",
    "\n",
    "> 操作步驟\n",
    "1. 點開雲端[連結](https://drive.google.com/file/d/1rTfeCpKXoQXI978QiTWC-AI1vwGvd5SU/view?usp=sharing)\n",
    "2. 點選右上角「新增雲端硬碟捷徑」\n",
    "3. 點選「我的雲端硬碟」\n",
    "4. 點選「新增捷徑」\n",
    "\n",
    "完成以上流程會在你的雲端硬碟中建立一個檔案的捷徑，接著我們在colab中取得權限即可使用。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jBY0b6zxI0r9"
   },
   "source": [
    "執行此段後點選出現的連結，允許授權後，複製授權碼，貼在空格中後按下ENTER，即完成與雲端硬碟連結。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 36
    },
    "id": "cCXepUIVe5iJ",
    "outputId": "96ca72d5-9b36-4f35-9ec7-899f28c16272"
   },
   "outputs": [],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cqO8DiB6VRQZ"
   },
   "source": [
    "## Unzip Data\n",
    "\n",
    "解壓縮 `flower_data.zip` 後可以發現裡面有兩個資料夾和三個csv檔。\n",
    "\n",
    "+ `train` : 存有五個資料夾分別是五個種類的花，資料夾內為花的照片。\n",
    "+ `test` : 資料夾中為未分類之測試集照片。\n",
    "+ `train.csv` : 讀取 train data 的順序、路徑與圖片所屬花別。\n",
    "+ `val.csv` : 讀取 validate data 的順序、路徑與圖片所屬花別。\n",
    "+ `test.csv` : 讀取 test data 的順序、路徑。\n",
    "\n",
    "其中`train`的圖片3112張，`val`的圖片778章，`test`的圖片433張。\n",
    "\n",
    "注意: 若有另外設定存放在雲端硬碟中的路徑，請記得本處路徑也須做更動。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "aGCZQxZSfONu"
   },
   "outputs": [],
   "source": [
    "!unzip -qq ./drive/My\\ Drive/flower_data.zip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# in our lab's server\n",
    "!unzip -qq flower_data.zip"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SYueKGdIHpho"
   },
   "source": [
    "## Loading the dataset\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xpG2DxEqHFD-"
   },
   "source": [
    "### Custom dataset\n",
    "\n",
    "繼承自定義資料集的框架 `torch.utils.data.Dataset`，主要實現 `__getitem__()` 和 `__len__()` 這兩個方法。\n",
    "\n",
    "常使用來做到設定資料位址、設定讀取方式、子資料集的標籤和轉換條件...等。\n",
    "\n",
    "See [torch.utils.data.Dataset](https://pytorch.org/docs/stable/data.html#torch.utils.data.Dataset) for more details"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "dwUE1E83_Iw8"
   },
   "outputs": [],
   "source": [
    "import csv\n",
    "import os\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "import torch\n",
    "\n",
    "class FlowerData(torch.utils.data.Dataset):\n",
    "    def __init__(self, csv_file, mode='train', transform=None):\n",
    "        \n",
    "        self.mode = mode # 'train', 'val' or 'test'\n",
    "        self.data_list = []\n",
    "        self.labels = []\n",
    "        self.transform = transform\n",
    "        \n",
    "        with open(csv_file, newline='') as csvfile:\n",
    "            reader = csv.DictReader(csvfile)\n",
    "            for row in reader:\n",
    "                self.data_list.append(row['file_path'])\n",
    "                if mode != 'test':\n",
    "                    self.labels.append(row['label'])\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "\n",
    "        data = Image.open(self.data_list[index])\n",
    "        if self.transform is not None:\n",
    "            data = self.transform(data)\n",
    "        if self.mode == 'test':\n",
    "            return data\n",
    "        label = torch.tensor(int(self.labels[index]))\n",
    "\n",
    "        return data, label\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5tQGLuVWnA-b"
   },
   "source": [
    "### Data augmentation \n",
    "\n",
    "Data augmentation are techniques used to increase the amount of data by adding slightly modified copies of already existing data or newly created synthetic data from existing data.\n",
    "\n",
    "Pytorch use `torchvision.transforms` to do data augmentation.\n",
    "[You can see all function here.](https://pytorch.org/docs/stable/torchvision/transforms.html)\n",
    "\n",
    "**NOTICE**: There are some operations may not be necessary for predict, so we should write one for train and one for others.\n",
    "\n",
    "(**Slide.07 page.49**)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "UIv1VyHXVNTo"
   },
   "outputs": [],
   "source": [
    "from torchvision import transforms\n",
    "# For TRAIN\n",
    "########################################################################\n",
    "#  TODO: use transforms.xxx method to do some data augmentation        #\n",
    "#  This one is for training, find the composition to get better result #\n",
    "########################################################################\n",
    "transforms_train = transforms.Compose([\n",
    "    transforms.Resize(240),\n",
    "    transforms.RandomResizedCrop(224),\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    transforms.RandomVerticalFlip(),\n",
    "    transforms.RandomRotation(30),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "])\n",
    "########################################################################\n",
    "#                           End of your code                           #\n",
    "########################################################################\n",
    "\n",
    "# For VAL, TEST\n",
    "########################################################################\n",
    "#  TODO: use transforms.xxx method to do some data augmentation        #\n",
    "#  This one is for validate and test,                                  #\n",
    "#  NOTICE some operation we usually not use in this part               #\n",
    "########################################################################\n",
    "transforms_test = transforms.Compose([\n",
    "    transforms.Resize(240),\n",
    "    transforms.CenterCrop(224),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "])\n",
    "########################################################################\n",
    "#                           End of your code                           #\n",
    "########################################################################"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7rYptn_YJlFX"
   },
   "source": [
    "### Instantiate dataset\n",
    "\n",
    "Let's instantiate three `FlowerData` class.\n",
    "+ dataset_train: for training.\n",
    "+ dataset_val: for validation.\n",
    "+ dataset_test: for tesing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "LKjsKyHhZZc6"
   },
   "outputs": [],
   "source": [
    "dataset_train = FlowerData('./data/train.csv', mode='train', transform=transforms_train)\n",
    "dataset_val = FlowerData('./data/val.csv', mode='val', transform=transforms_test)\n",
    "dataset_test = FlowerData('./data/test.csv', mode='test', transform=transforms_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 54
    },
    "id": "ZAeQQEEISCJJ",
    "outputId": "c9ec9a06-17ff-48a4-a9df-776987ef5720"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The first image's shape in dataset_train : torch.Size([3, 224, 224])\n",
      "There are 3112 images in dataset_train.\n",
      "There are 433 images in dataset_test.\n"
     ]
    }
   ],
   "source": [
    "print(\"The first image's shape in dataset_train :\", dataset_train.__getitem__(0)[0].size())\n",
    "print(\"There are\", len(dataset_train), \"images in dataset_train.\")\n",
    "print(\"There are\", len(dataset_test), \"images in dataset_test.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vORA6qkfIj1U"
   },
   "source": [
    "### `DataLoader`\n",
    "\n",
    "`torch.utils.data.DataLoader` define how to sample from `dataset` and some other function like:\n",
    "+ `shuffle` : set to `True` to have the data reshuffled at every epoch\n",
    "+ `batch_size` : how many samples per batch to load\n",
    "\n",
    "See [torch.utils.data.DataLoader](https://pytorch.org/docs/stable/data.html#torch.utils.data.DataLoader) for more details"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "RmgA5nYT3XQZ"
   },
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "\n",
    "train_loader = DataLoader(dataset_train, batch_size=32, num_workers=16, shuffle=True)\n",
    "val_loader = DataLoader(dataset_val, batch_size=32, num_workers=16, shuffle=False)\n",
    "test_loader = DataLoader(dataset_test, batch_size=32, num_workers=16, shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "U4atwzT3aPi3"
   },
   "source": [
    "Finally! We have made all data prepared.  \n",
    "Let's go develop our model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "87KYcWknS95z"
   },
   "source": [
    "# Implement CNN using PyTorch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "OH_4NKB9dsZ3"
   },
   "source": [
    "### Define a Convolutional Neural Network\n",
    "\n",
    "Try to design and train a deep convolutional network from scratch to predict the class label of a flower image. \n",
    "\n",
    "You can refer to last assignment about image_classifier, and try to go deep and use more method for better model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "S_HxxEfdO2Ss"
   },
   "outputs": [],
   "source": [
    "# handmade resnext by myself\n",
    "import torch.nn as nn \n",
    "import torch.nn.functional as F\n",
    "\n",
    "# component conv1x1 and 3x3 of resnet\n",
    "def conv1x1(in_chn, out_chn, stride=1):\n",
    "    return nn.Conv2d(in_chn, out_chn, kernel_size=1,\n",
    "                     stride=stride, bias=False)\n",
    "\n",
    "def conv3x3(in_chn, out_chn, stride=1, groups=1, dilation=1):\n",
    "    return nn.Conv2d(in_chn, out_chn, kernel_size=3,\n",
    "                     stride=stride, padding=dilation,\n",
    "                     groups=groups, bias=False, dilation=dilation)\n",
    "                    # groups is keypoint for renext\n",
    "\n",
    "# I also try resnet34 with BasicBlock\n",
    "# class BasicBlock(nn.Module):\n",
    "\n",
    "# Block of resnet50, resnet101, resnet152...\n",
    "class BottleNeck(nn.Module):\n",
    "    expansion = 4\n",
    "\n",
    "    def __init__(self, in_chn, out_chn, stride=1, downsample=None, groups=1, base_width=64):\n",
    "        super().__init__()\n",
    "\n",
    "        width = int(out_chn * (base_width / 64.)) * groups\n",
    "        self.layers = nn.Sequential(\n",
    "            conv1x1(in_chn, width),\n",
    "            nn.BatchNorm2d(width),\n",
    "            nn.ReLU(inplace=True),\n",
    "            \n",
    "            conv3x3(width, width, stride=stride, groups=groups),\n",
    "            nn.BatchNorm2d(width),\n",
    "            nn.ReLU(inplace=True),\n",
    "            \n",
    "            conv1x1(width, out_chn * self.expansion),\n",
    "            nn.BatchNorm2d(out_chn * self.expansion),\n",
    "        )\n",
    "        self.downsample = downsample\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = self.layers(x)\n",
    "        \n",
    "        identity = x\n",
    "        if self.downsample is not None:\n",
    "            identity = self.downsample(x)\n",
    "\n",
    "        out += identity\n",
    "        out = F.relu(out)\n",
    "\n",
    "        return out\n",
    "\n",
    "# ResNext Module\n",
    "class ResNext50(nn.Module): \n",
    "    def __init__(self): \n",
    "        super().__init__()\n",
    "        ########################################################################\n",
    "        #     TODO: use nn.xxx method to generate a CNN model part             #\n",
    "        ########################################################################\n",
    "        # define core model\n",
    "        # resnet50 32x4d setting\n",
    "        block = BottleNeck\n",
    "        layers = [3, 4, 6, 3]\n",
    "        pre = nn.Sequential(\n",
    "            nn.Conv2d(3, 64, kernel_size=7, stride=2,\n",
    "                      padding=3, bias=False),\n",
    "            nn.BatchNorm2d(64),\n",
    "            nn.ReLU(inplace=True),\n",
    "        )\n",
    "        \n",
    "        self.in_chn = 64\n",
    "        self.groups = 32 # default 1\n",
    "        self.width_per_group = 4 # deafult 64\n",
    "        self.base_width = self.width_per_group\n",
    "\n",
    "        self.pre = pre\n",
    "        self.max_pool = nn.MaxPool2d(kernel_size=3, stride=2, padding=1)\n",
    "\n",
    "        # stack blocks and layers\n",
    "        self.layer1 = self._make_layer(block, 64, layers[0])\n",
    "        self.layer2 = self._make_layer(block, 128, layers[1], stride=2)\n",
    "        self.layer3 = self._make_layer(block, 256, layers[2], stride=2)\n",
    "        self.layer4 = self._make_layer(block, 512, layers[3], stride=2)\n",
    "\n",
    "        self.avg_pool = nn.AdaptiveAvgPool2d((1, 1))\n",
    "        self.fc = nn.Sequential(\n",
    "            nn.Linear(512 * block.expansion, 256),\n",
    "            nn.ReLU(True),\n",
    "            nn.Dropout(),\n",
    "            nn.Linear(256, 5),\n",
    "        )\n",
    "\n",
    "    # build several blocks together\n",
    "    def _make_layer(self, block, out_chn, block_num, stride=1):\n",
    "        downsample = None\n",
    "\n",
    "        if stride != 1 or self.in_chn != out_chn * block.expansion:\n",
    "            downsample = nn.Sequential(\n",
    "                conv1x1(self.in_chn, out_chn * block.expansion, stride),\n",
    "                nn.BatchNorm2d(out_chn * block.expansion),\n",
    "            )\n",
    "\n",
    "        layers = []\n",
    "        layers.append(block(self.in_chn, out_chn, stride,\n",
    "                            downsample, groups=self.groups,\n",
    "                            base_width=self.base_width))\n",
    "        self.in_chn = out_chn * block.expansion\n",
    "        for _ in range(1, block_num):\n",
    "            layers.append(block(self.in_chn, out_chn,\n",
    "                                groups=self.groups,\n",
    "                                base_width=self.base_width))\n",
    "\n",
    "        return nn.Sequential(*layers)\n",
    "        ########################################################################\n",
    "        #                           End of your code                           #\n",
    "        ########################################################################\n",
    "\n",
    "    def forward(self, x): \n",
    "        if not isinstance(x, torch.Tensor):\n",
    "            x = torch.Tensor(x)\n",
    "        ########################################################################\n",
    "        #     TODO: forward your model and get output                          #\n",
    "        ########################################################################\n",
    "        x = self.pre(x)\n",
    "        x = self.max_pool(x)\n",
    "\n",
    "        x = self.layer1(x)\n",
    "        x = self.layer2(x)\n",
    "        x = self.layer3(x)\n",
    "        x = self.layer4(x)\n",
    "\n",
    "        out = self.avg_pool(x)\n",
    "        out = torch.flatten(out, 1)\n",
    "        out = self.fc(out)\n",
    "        ########################################################################\n",
    "        #                           End of your code                           #\n",
    "        ########################################################################\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 333
    },
    "id": "lP55C9Xjydws",
    "outputId": "35263053-2ed8-48d2-c1ff-ee5fc4aa87c7"
   },
   "outputs": [],
   "source": [
    "model = ResNext50()\n",
    "model = model.cuda()\n",
    "# print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------------------------------\n",
      "        Layer (type)               Output Shape         Param #\n",
      "================================================================\n",
      "            Conv2d-1         [-1, 64, 112, 112]           9,408\n",
      "       BatchNorm2d-2         [-1, 64, 112, 112]             128\n",
      "              ReLU-3         [-1, 64, 112, 112]               0\n",
      "         MaxPool2d-4           [-1, 64, 56, 56]               0\n",
      "            Conv2d-5          [-1, 128, 56, 56]           8,192\n",
      "       BatchNorm2d-6          [-1, 128, 56, 56]             256\n",
      "              ReLU-7          [-1, 128, 56, 56]               0\n",
      "            Conv2d-8          [-1, 128, 56, 56]           4,608\n",
      "       BatchNorm2d-9          [-1, 128, 56, 56]             256\n",
      "             ReLU-10          [-1, 128, 56, 56]               0\n",
      "           Conv2d-11          [-1, 256, 56, 56]          32,768\n",
      "      BatchNorm2d-12          [-1, 256, 56, 56]             512\n",
      "           Conv2d-13          [-1, 256, 56, 56]          16,384\n",
      "      BatchNorm2d-14          [-1, 256, 56, 56]             512\n",
      "       BottleNeck-15          [-1, 256, 56, 56]               0\n",
      "           Conv2d-16          [-1, 128, 56, 56]          32,768\n",
      "      BatchNorm2d-17          [-1, 128, 56, 56]             256\n",
      "             ReLU-18          [-1, 128, 56, 56]               0\n",
      "           Conv2d-19          [-1, 128, 56, 56]           4,608\n",
      "      BatchNorm2d-20          [-1, 128, 56, 56]             256\n",
      "             ReLU-21          [-1, 128, 56, 56]               0\n",
      "           Conv2d-22          [-1, 256, 56, 56]          32,768\n",
      "      BatchNorm2d-23          [-1, 256, 56, 56]             512\n",
      "       BottleNeck-24          [-1, 256, 56, 56]               0\n",
      "           Conv2d-25          [-1, 128, 56, 56]          32,768\n",
      "      BatchNorm2d-26          [-1, 128, 56, 56]             256\n",
      "             ReLU-27          [-1, 128, 56, 56]               0\n",
      "           Conv2d-28          [-1, 128, 56, 56]           4,608\n",
      "      BatchNorm2d-29          [-1, 128, 56, 56]             256\n",
      "             ReLU-30          [-1, 128, 56, 56]               0\n",
      "           Conv2d-31          [-1, 256, 56, 56]          32,768\n",
      "      BatchNorm2d-32          [-1, 256, 56, 56]             512\n",
      "       BottleNeck-33          [-1, 256, 56, 56]               0\n",
      "           Conv2d-34          [-1, 256, 56, 56]          65,536\n",
      "      BatchNorm2d-35          [-1, 256, 56, 56]             512\n",
      "             ReLU-36          [-1, 256, 56, 56]               0\n",
      "           Conv2d-37          [-1, 256, 28, 28]          18,432\n",
      "      BatchNorm2d-38          [-1, 256, 28, 28]             512\n",
      "             ReLU-39          [-1, 256, 28, 28]               0\n",
      "           Conv2d-40          [-1, 512, 28, 28]         131,072\n",
      "      BatchNorm2d-41          [-1, 512, 28, 28]           1,024\n",
      "           Conv2d-42          [-1, 512, 28, 28]         131,072\n",
      "      BatchNorm2d-43          [-1, 512, 28, 28]           1,024\n",
      "       BottleNeck-44          [-1, 512, 28, 28]               0\n",
      "           Conv2d-45          [-1, 256, 28, 28]         131,072\n",
      "      BatchNorm2d-46          [-1, 256, 28, 28]             512\n",
      "             ReLU-47          [-1, 256, 28, 28]               0\n",
      "           Conv2d-48          [-1, 256, 28, 28]          18,432\n",
      "      BatchNorm2d-49          [-1, 256, 28, 28]             512\n",
      "             ReLU-50          [-1, 256, 28, 28]               0\n",
      "           Conv2d-51          [-1, 512, 28, 28]         131,072\n",
      "      BatchNorm2d-52          [-1, 512, 28, 28]           1,024\n",
      "       BottleNeck-53          [-1, 512, 28, 28]               0\n",
      "           Conv2d-54          [-1, 256, 28, 28]         131,072\n",
      "      BatchNorm2d-55          [-1, 256, 28, 28]             512\n",
      "             ReLU-56          [-1, 256, 28, 28]               0\n",
      "           Conv2d-57          [-1, 256, 28, 28]          18,432\n",
      "      BatchNorm2d-58          [-1, 256, 28, 28]             512\n",
      "             ReLU-59          [-1, 256, 28, 28]               0\n",
      "           Conv2d-60          [-1, 512, 28, 28]         131,072\n",
      "      BatchNorm2d-61          [-1, 512, 28, 28]           1,024\n",
      "       BottleNeck-62          [-1, 512, 28, 28]               0\n",
      "           Conv2d-63          [-1, 256, 28, 28]         131,072\n",
      "      BatchNorm2d-64          [-1, 256, 28, 28]             512\n",
      "             ReLU-65          [-1, 256, 28, 28]               0\n",
      "           Conv2d-66          [-1, 256, 28, 28]          18,432\n",
      "      BatchNorm2d-67          [-1, 256, 28, 28]             512\n",
      "             ReLU-68          [-1, 256, 28, 28]               0\n",
      "           Conv2d-69          [-1, 512, 28, 28]         131,072\n",
      "      BatchNorm2d-70          [-1, 512, 28, 28]           1,024\n",
      "       BottleNeck-71          [-1, 512, 28, 28]               0\n",
      "           Conv2d-72          [-1, 512, 28, 28]         262,144\n",
      "      BatchNorm2d-73          [-1, 512, 28, 28]           1,024\n",
      "             ReLU-74          [-1, 512, 28, 28]               0\n",
      "           Conv2d-75          [-1, 512, 14, 14]          73,728\n",
      "      BatchNorm2d-76          [-1, 512, 14, 14]           1,024\n",
      "             ReLU-77          [-1, 512, 14, 14]               0\n",
      "           Conv2d-78         [-1, 1024, 14, 14]         524,288\n",
      "      BatchNorm2d-79         [-1, 1024, 14, 14]           2,048\n",
      "           Conv2d-80         [-1, 1024, 14, 14]         524,288\n",
      "      BatchNorm2d-81         [-1, 1024, 14, 14]           2,048\n",
      "       BottleNeck-82         [-1, 1024, 14, 14]               0\n",
      "           Conv2d-83          [-1, 512, 14, 14]         524,288\n",
      "      BatchNorm2d-84          [-1, 512, 14, 14]           1,024\n",
      "             ReLU-85          [-1, 512, 14, 14]               0\n",
      "           Conv2d-86          [-1, 512, 14, 14]          73,728\n",
      "      BatchNorm2d-87          [-1, 512, 14, 14]           1,024\n",
      "             ReLU-88          [-1, 512, 14, 14]               0\n",
      "           Conv2d-89         [-1, 1024, 14, 14]         524,288\n",
      "      BatchNorm2d-90         [-1, 1024, 14, 14]           2,048\n",
      "       BottleNeck-91         [-1, 1024, 14, 14]               0\n",
      "           Conv2d-92          [-1, 512, 14, 14]         524,288\n",
      "      BatchNorm2d-93          [-1, 512, 14, 14]           1,024\n",
      "             ReLU-94          [-1, 512, 14, 14]               0\n",
      "           Conv2d-95          [-1, 512, 14, 14]          73,728\n",
      "      BatchNorm2d-96          [-1, 512, 14, 14]           1,024\n",
      "             ReLU-97          [-1, 512, 14, 14]               0\n",
      "           Conv2d-98         [-1, 1024, 14, 14]         524,288\n",
      "      BatchNorm2d-99         [-1, 1024, 14, 14]           2,048\n",
      "      BottleNeck-100         [-1, 1024, 14, 14]               0\n",
      "          Conv2d-101          [-1, 512, 14, 14]         524,288\n",
      "     BatchNorm2d-102          [-1, 512, 14, 14]           1,024\n",
      "            ReLU-103          [-1, 512, 14, 14]               0\n",
      "          Conv2d-104          [-1, 512, 14, 14]          73,728\n",
      "     BatchNorm2d-105          [-1, 512, 14, 14]           1,024\n",
      "            ReLU-106          [-1, 512, 14, 14]               0\n",
      "          Conv2d-107         [-1, 1024, 14, 14]         524,288\n",
      "     BatchNorm2d-108         [-1, 1024, 14, 14]           2,048\n",
      "      BottleNeck-109         [-1, 1024, 14, 14]               0\n",
      "          Conv2d-110          [-1, 512, 14, 14]         524,288\n",
      "     BatchNorm2d-111          [-1, 512, 14, 14]           1,024\n",
      "            ReLU-112          [-1, 512, 14, 14]               0\n",
      "          Conv2d-113          [-1, 512, 14, 14]          73,728\n",
      "     BatchNorm2d-114          [-1, 512, 14, 14]           1,024\n",
      "            ReLU-115          [-1, 512, 14, 14]               0\n",
      "          Conv2d-116         [-1, 1024, 14, 14]         524,288\n",
      "     BatchNorm2d-117         [-1, 1024, 14, 14]           2,048\n",
      "      BottleNeck-118         [-1, 1024, 14, 14]               0\n",
      "          Conv2d-119          [-1, 512, 14, 14]         524,288\n",
      "     BatchNorm2d-120          [-1, 512, 14, 14]           1,024\n",
      "            ReLU-121          [-1, 512, 14, 14]               0\n",
      "          Conv2d-122          [-1, 512, 14, 14]          73,728\n",
      "     BatchNorm2d-123          [-1, 512, 14, 14]           1,024\n",
      "            ReLU-124          [-1, 512, 14, 14]               0\n",
      "          Conv2d-125         [-1, 1024, 14, 14]         524,288\n",
      "     BatchNorm2d-126         [-1, 1024, 14, 14]           2,048\n",
      "      BottleNeck-127         [-1, 1024, 14, 14]               0\n",
      "          Conv2d-128         [-1, 1024, 14, 14]       1,048,576\n",
      "     BatchNorm2d-129         [-1, 1024, 14, 14]           2,048\n",
      "            ReLU-130         [-1, 1024, 14, 14]               0\n",
      "          Conv2d-131           [-1, 1024, 7, 7]         294,912\n",
      "     BatchNorm2d-132           [-1, 1024, 7, 7]           2,048\n",
      "            ReLU-133           [-1, 1024, 7, 7]               0\n",
      "          Conv2d-134           [-1, 2048, 7, 7]       2,097,152\n",
      "     BatchNorm2d-135           [-1, 2048, 7, 7]           4,096\n",
      "          Conv2d-136           [-1, 2048, 7, 7]       2,097,152\n",
      "     BatchNorm2d-137           [-1, 2048, 7, 7]           4,096\n",
      "      BottleNeck-138           [-1, 2048, 7, 7]               0\n",
      "          Conv2d-139           [-1, 1024, 7, 7]       2,097,152\n",
      "     BatchNorm2d-140           [-1, 1024, 7, 7]           2,048\n",
      "            ReLU-141           [-1, 1024, 7, 7]               0\n",
      "          Conv2d-142           [-1, 1024, 7, 7]         294,912\n",
      "     BatchNorm2d-143           [-1, 1024, 7, 7]           2,048\n",
      "            ReLU-144           [-1, 1024, 7, 7]               0\n",
      "          Conv2d-145           [-1, 2048, 7, 7]       2,097,152\n",
      "     BatchNorm2d-146           [-1, 2048, 7, 7]           4,096\n",
      "      BottleNeck-147           [-1, 2048, 7, 7]               0\n",
      "          Conv2d-148           [-1, 1024, 7, 7]       2,097,152\n",
      "     BatchNorm2d-149           [-1, 1024, 7, 7]           2,048\n",
      "            ReLU-150           [-1, 1024, 7, 7]               0\n",
      "          Conv2d-151           [-1, 1024, 7, 7]         294,912\n",
      "     BatchNorm2d-152           [-1, 1024, 7, 7]           2,048\n",
      "            ReLU-153           [-1, 1024, 7, 7]               0\n",
      "          Conv2d-154           [-1, 2048, 7, 7]       2,097,152\n",
      "     BatchNorm2d-155           [-1, 2048, 7, 7]           4,096\n",
      "      BottleNeck-156           [-1, 2048, 7, 7]               0\n",
      "AdaptiveAvgPool2d-157           [-1, 2048, 1, 1]               0\n",
      "          Linear-158                  [-1, 256]         524,544\n",
      "            ReLU-159                  [-1, 256]               0\n",
      "         Dropout-160                  [-1, 256]               0\n",
      "          Linear-161                    [-1, 5]           1,285\n",
      "================================================================\n",
      "Total params: 23,505,733\n",
      "Trainable params: 23,505,733\n",
      "Non-trainable params: 0\n",
      "----------------------------------------------------------------\n",
      "Input size (MB): 0.57\n",
      "Forward/backward pass size (MB): 319.67\n",
      "Params size (MB): 89.67\n",
      "Estimated Total Size (MB): 409.91\n",
      "----------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# using torchsummary check network and params\n",
    "from torchsummary import summary\n",
    "summary(model, input_size=(3, 224, 224))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "83imqLRQ0v7M"
   },
   "source": [
    "We have made our model!  \n",
    "Next, PyTorch also provide many utility function(loss, optmizer...etc).  \n",
    "You can define them in one-line."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FtOkPO6Ga0Fw"
   },
   "source": [
    "### Define loss and optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "id": "IoePct00RIFY"
   },
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "################################################################################\n",
    "# TODO: Define loss and optmizer functions                                     #\n",
    "# Try any loss or optimizer function and learning rate to get better result    #\n",
    "# hint: torch.nn and torch.optim                                               #\n",
    "################################################################################\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.SGD(model.parameters(), lr=1e-2, momentum=0.9)\n",
    "################################################################################\n",
    "#                               End of your code                               #\n",
    "################################################################################\n",
    "# criterion = criterion.cuda()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zle9KuFcbwMP"
   },
   "source": [
    "### Train the model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bZFxE7Y9iLfl"
   },
   "source": [
    "#### Train function\n",
    "Let's define train function.  \n",
    "It will iterate input data 1 epoch and update model with optmizer.  \n",
    "Finally, calculate mean loss and total accuracy.\n",
    "\n",
    "Hint: [torch.max()](https://pytorch.org/docs/stable/generated/torch.max.html#torch-max)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "id": "VM93brDshO6E"
   },
   "outputs": [],
   "source": [
    "def train(input_data, model, criterion, optimizer):\n",
    "    '''\n",
    "    Argement:\n",
    "    input_data -- iterable data, typr torch.utils.data.Dataloader is prefer\n",
    "    model -- nn.Module, model contain forward to predict output\n",
    "    criterion -- loss function, used to evaluate goodness of model\n",
    "    optimizer -- optmizer function, method for weight updating\n",
    "    '''\n",
    "    model.train()\n",
    "    loss_list = []\n",
    "    total_count = 0\n",
    "    acc_count = 0\n",
    "    for i, data in enumerate(input_data, 0):\n",
    "        images, labels = data[0].cuda(), data[1].cuda()\n",
    "        \n",
    "        ########################################################################\n",
    "        # TODO: Forward, backward and optimize                                 #\n",
    "        # 1. zero the parameter gradients                                      #\n",
    "        # 2. process input through the network                                 #\n",
    "        # 3. compute the loss                                                  #\n",
    "        # 4. propagate gradients back into the network’s parameters            #\n",
    "        # 5. Update the weights of the network                                 #\n",
    "        ########################################################################\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(images)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        ########################################################################\n",
    "        #                           End of your code                           #\n",
    "        ########################################################################\n",
    "\n",
    "\n",
    "        ########################################################################\n",
    "        # TODO: Get the counts of correctly classified images                  #\n",
    "        # 1. get the model predicted result                                    #\n",
    "        # 2. sum the number of this batch predicted images                     #\n",
    "        # 3. sum the number of correctly classified                            #\n",
    "        # 4. save this batch's loss into loss_list                             #\n",
    "        # dimension of outputs: [batch_size, number of classes]                #\n",
    "        # Hint 1: use outputs.data to get no auto_grad                         #\n",
    "        # Hint 2: use torch.max()                                              #\n",
    "        ########################################################################\n",
    "        pred = torch.argmax(outputs, dim=1)\n",
    "        acc_count += (pred == labels).sum().item()\n",
    "        total_count += images.shape[0]\n",
    "        loss_list.append(loss.item())\n",
    "        ########################################################################\n",
    "        #                           End of your code                           #\n",
    "        ########################################################################\n",
    "\n",
    "    # Compute this epoch accuracy and loss\n",
    "    acc = acc_count / total_count\n",
    "    loss = sum(loss_list) / len(loss_list)\n",
    "    return acc, loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KmDy1GTq_H2a"
   },
   "source": [
    "#### Validate function\n",
    "Next part is validate function.  \n",
    "It works as training function without optmizer and weight-updating part."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "id": "USzbBgGEoTRu"
   },
   "outputs": [],
   "source": [
    "def val(input_data, model, criterion):\n",
    "    model.eval()\n",
    "    \n",
    "    loss_list = []\n",
    "    total_count = 0\n",
    "    acc_count = 0\n",
    "    with torch.no_grad():\n",
    "        for data in input_data:\n",
    "            images, labels = data[0].cuda(), data[1].cuda()\n",
    "\n",
    "            ####################################################################\n",
    "            # TODO: Get the predicted result and loss                          #\n",
    "            # 1. process input through the network                             #\n",
    "            # 2. compute the loss                                              #\n",
    "            # 3. get the model predicted result                                #\n",
    "            # 4. get the counts of correctly classified images                 #\n",
    "            # 5. save this batch's loss into loss_list                         #\n",
    "            ####################################################################\n",
    "            outputs = model(images)\n",
    "            loss = criterion(outputs, labels)\n",
    "            \n",
    "            pred = torch.argmax(outputs, dim=1)\n",
    "            acc_count += (pred == labels).sum().item()\n",
    "            total_count += images.shape[0]\n",
    "            loss_list.append(loss.item())\n",
    "            ####################################################################\n",
    "            #                         End of your code                         #\n",
    "            ####################################################################\n",
    "\n",
    "    acc = acc_count / total_count\n",
    "    loss = sum(loss_list) / len(loss_list)\n",
    "    return acc, loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "knXu74jCiuxP"
   },
   "source": [
    "#### Training in a loop\n",
    "Call train and test function in a loop.  \n",
    "Take a break and wait."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 296
    },
    "id": "rcVulKkFJRtI",
    "outputId": "13055c06-941e-4ff0-86da-2840e1103aed"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==================== Epoch 51 ====================\n",
      "Train Acc: 0.697301 Train Loss: 0.778310\n",
      "  Val Acc: 0.703085   Val Loss: 0.734941\n",
      "==================== Epoch 52 ====================\n",
      "Train Acc: 0.693445 Train Loss: 0.786196\n",
      "  Val Acc: 0.696658   Val Loss: 0.831493\n",
      "==================== Epoch 53 ====================\n",
      "Train Acc: 0.702121 Train Loss: 0.771395\n",
      "  Val Acc: 0.724936   Val Loss: 0.757244\n",
      "==================== Epoch 54 ====================\n",
      "Train Acc: 0.669023 Train Loss: 0.857340\n",
      "  Val Acc: 0.712082   Val Loss: 0.785891\n",
      "==================== Epoch 55 ====================\n",
      "Train Acc: 0.700514 Train Loss: 0.776444\n",
      "  Val Acc: 0.664524   Val Loss: 0.842230\n",
      "==================== Epoch 56 ====================\n",
      "Train Acc: 0.710797 Train Loss: 0.752816\n",
      "  Val Acc: 0.733933   Val Loss: 0.754134\n",
      "==================== Epoch 57 ====================\n",
      "Train Acc: 0.709190 Train Loss: 0.753632\n",
      "  Val Acc: 0.745501   Val Loss: 0.717792\n",
      "==================== Epoch 58 ====================\n",
      "Train Acc: 0.720437 Train Loss: 0.734420\n",
      "  Val Acc: 0.745501   Val Loss: 0.686443\n",
      "==================== Epoch 59 ====================\n",
      "Train Acc: 0.714653 Train Loss: 0.743297\n",
      "  Val Acc: 0.741645   Val Loss: 0.809046\n",
      "==================== Epoch 60 ====================\n",
      "Train Acc: 0.723329 Train Loss: 0.735843\n",
      "  Val Acc: 0.767352   Val Loss: 0.673244\n",
      "==================== Epoch 61 ====================\n",
      "Train Acc: 0.734576 Train Loss: 0.709893\n",
      "  Val Acc: 0.751928   Val Loss: 0.688879\n",
      "==================== Epoch 62 ====================\n",
      "Train Acc: 0.726542 Train Loss: 0.710792\n",
      "  Val Acc: 0.750643   Val Loss: 0.752098\n",
      "==================== Epoch 63 ====================\n",
      "Train Acc: 0.731362 Train Loss: 0.713540\n",
      "  Val Acc: 0.730077   Val Loss: 0.712238\n",
      "==================== Epoch 64 ====================\n",
      "Train Acc: 0.725900 Train Loss: 0.701573\n",
      "  Val Acc: 0.767352   Val Loss: 0.686549\n",
      "==================== Epoch 65 ====================\n",
      "Train Acc: 0.739075 Train Loss: 0.687024\n",
      "  Val Acc: 0.772494   Val Loss: 0.751430\n",
      "==================== Epoch 66 ====================\n",
      "Train Acc: 0.751607 Train Loss: 0.683864\n",
      "  Val Acc: 0.764781   Val Loss: 0.697027\n",
      "==================== Epoch 67 ====================\n",
      "Train Acc: 0.735219 Train Loss: 0.702720\n",
      "  Val Acc: 0.760925   Val Loss: 0.661462\n",
      "==================== Epoch 68 ====================\n",
      "Train Acc: 0.723972 Train Loss: 0.749279\n",
      "  Val Acc: 0.737789   Val Loss: 0.776566\n",
      "==================== Epoch 69 ====================\n",
      "Train Acc: 0.729434 Train Loss: 0.726770\n",
      "  Val Acc: 0.764781   Val Loss: 0.643485\n",
      "==================== Epoch 70 ====================\n",
      "Train Acc: 0.735219 Train Loss: 0.695703\n",
      "  Val Acc: 0.757069   Val Loss: 0.688480\n",
      "==================== Epoch 71 ====================\n",
      "Train Acc: 0.752249 Train Loss: 0.669531\n",
      "  Val Acc: 0.775064   Val Loss: 0.638746\n",
      "==================== Epoch 72 ====================\n",
      "Train Acc: 0.761568 Train Loss: 0.661356\n",
      "  Val Acc: 0.757069   Val Loss: 0.667777\n",
      "==================== Epoch 73 ====================\n",
      "Train Acc: 0.745180 Train Loss: 0.693918\n",
      "  Val Acc: 0.736504   Val Loss: 1.623922\n",
      "==================== Epoch 74 ====================\n",
      "Train Acc: 0.759640 Train Loss: 0.643641\n",
      "  Val Acc: 0.749357   Val Loss: 0.664824\n",
      "==================== Epoch 75 ====================\n",
      "Train Acc: 0.749357 Train Loss: 0.680158\n",
      "  Val Acc: 0.766067   Val Loss: 0.642843\n",
      "==================== Epoch 76 ====================\n",
      "Train Acc: 0.747108 Train Loss: 0.677682\n",
      "  Val Acc: 0.775064   Val Loss: 0.619492\n",
      "==================== Epoch 77 ====================\n",
      "Train Acc: 0.758676 Train Loss: 0.661178\n",
      "  Val Acc: 0.757069   Val Loss: 0.655020\n",
      "==================== Epoch 78 ====================\n",
      "Train Acc: 0.753213 Train Loss: 0.649565\n",
      "  Val Acc: 0.721080   Val Loss: 0.859877\n",
      "==================== Epoch 79 ====================\n",
      "Train Acc: 0.770244 Train Loss: 0.617575\n",
      "  Val Acc: 0.772494   Val Loss: 0.645541\n",
      "==================== Epoch 80 ====================\n",
      "Train Acc: 0.764781 Train Loss: 0.630295\n",
      "  Val Acc: 0.778920   Val Loss: 0.662601\n",
      "==================== Epoch 81 ====================\n",
      "Train Acc: 0.761247 Train Loss: 0.639806\n",
      "  Val Acc: 0.787918   Val Loss: 0.649637\n",
      "==================== Epoch 82 ====================\n",
      "Train Acc: 0.764781 Train Loss: 0.613530\n",
      "  Val Acc: 0.740360   Val Loss: 0.712438\n",
      "==================== Epoch 83 ====================\n",
      "Train Acc: 0.781491 Train Loss: 0.596172\n",
      "  Val Acc: 0.764781   Val Loss: 0.684320\n",
      "==================== Epoch 84 ====================\n",
      "Train Acc: 0.775064 Train Loss: 0.631179\n",
      "  Val Acc: 0.767352   Val Loss: 0.668601\n",
      "==================== Epoch 85 ====================\n",
      "Train Acc: 0.758033 Train Loss: 0.639889\n",
      "  Val Acc: 0.790488   Val Loss: 0.563743\n",
      "==================== Epoch 86 ====================\n",
      "Train Acc: 0.785026 Train Loss: 0.577132\n",
      "  Val Acc: 0.766067   Val Loss: 0.743989\n",
      "==================== Epoch 87 ====================\n",
      "Train Acc: 0.782776 Train Loss: 0.603979\n",
      "  Val Acc: 0.793059   Val Loss: 0.567954\n",
      "==================== Epoch 88 ====================\n",
      "Train Acc: 0.769602 Train Loss: 0.620016\n",
      "  Val Acc: 0.793059   Val Loss: 0.645771\n",
      "==================== Epoch 89 ====================\n",
      "Train Acc: 0.780206 Train Loss: 0.568250\n",
      "  Val Acc: 0.772494   Val Loss: 0.697269\n",
      "==================== Epoch 90 ====================\n",
      "Train Acc: 0.780848 Train Loss: 0.603253\n",
      "  Val Acc: 0.786632   Val Loss: 0.611590\n",
      "==================== Epoch 91 ====================\n",
      "Train Acc: 0.798522 Train Loss: 0.560500\n",
      "  Val Acc: 0.789203   Val Loss: 0.669425\n",
      "==================== Epoch 92 ====================\n",
      "Train Acc: 0.791131 Train Loss: 0.569511\n",
      "  Val Acc: 0.780206   Val Loss: 0.657111\n",
      "==================== Epoch 93 ====================\n",
      "Train Acc: 0.767995 Train Loss: 0.636980\n",
      "  Val Acc: 0.796915   Val Loss: 0.601787\n",
      "==================== Epoch 94 ====================\n",
      "Train Acc: 0.768959 Train Loss: 0.644433\n",
      "  Val Acc: 0.768638   Val Loss: 0.661245\n",
      "==================== Epoch 95 ====================\n",
      "Train Acc: 0.785026 Train Loss: 0.590392\n",
      "  Val Acc: 0.793059   Val Loss: 0.608019\n",
      "==================== Epoch 96 ====================\n",
      "Train Acc: 0.785990 Train Loss: 0.570583\n",
      "  Val Acc: 0.785347   Val Loss: 0.692405\n",
      "==================== Epoch 97 ====================\n",
      "Train Acc: 0.789524 Train Loss: 0.555958\n",
      "  Val Acc: 0.758355   Val Loss: 0.741549\n",
      "==================== Epoch 98 ====================\n",
      "Train Acc: 0.792095 Train Loss: 0.578882\n",
      "  Val Acc: 0.818766   Val Loss: 0.528298\n",
      "==================== Epoch 99 ====================\n",
      "Train Acc: 0.782134 Train Loss: 0.588918\n",
      "  Val Acc: 0.808483   Val Loss: 0.552410\n",
      "==================== Epoch 100 ====================\n",
      "Train Acc: 0.791452 Train Loss: 0.575302\n",
      "  Val Acc: 0.816195   Val Loss: 0.565708\n"
     ]
    }
   ],
   "source": [
    "################################################################################\n",
    "# You can adjust those hyper parameters to loop for max_epochs times           #\n",
    "################################################################################\n",
    "max_epochs = 100\n",
    "log_interval = 1 # print acc and loss in per log_interval time\n",
    "# threshold to save checkpoints\n",
    "max_loss = 0.7\n",
    "max_acc = 0.7\n",
    "################################################################################\n",
    "#                               End of your code                               #\n",
    "################################################################################\n",
    "train_acc_list = []\n",
    "train_loss_list = []\n",
    "val_acc_list = []\n",
    "val_loss_list = []\n",
    "\n",
    "for epoch in range(51, max_epochs + 1):\n",
    "    train_acc, train_loss = train(train_loader, model, criterion, optimizer)\n",
    "    val_acc, val_loss = val(val_loader, model, criterion)\n",
    "\n",
    "    train_acc_list.append(train_acc)\n",
    "    train_loss_list.append(train_loss)\n",
    "    val_acc_list.append(val_acc)\n",
    "    val_loss_list.append(val_loss)\n",
    "    \n",
    "    # if get lower loss, save checkpoint\n",
    "    if val_loss < max_loss:\n",
    "        max_loss = val_loss\n",
    "        torch.save(model.state_dict(), 'res_e{:03}_val_loss_{:.02}.pt'.format(epoch, val_loss))\n",
    "    # if get higher acc, save checkpoint\n",
    "    if val_acc > max_acc:\n",
    "        max_acc = val_acc\n",
    "        torch.save(model.state_dict(), 'res_acc_e{:03}_val_acc_{:.02}.pt'.format(epoch, val_acc))\n",
    "    if epoch % log_interval == 0:\n",
    "        print('=' * 20, 'Epoch', epoch, '=' * 20)\n",
    "        print('Train Acc: {:.6f} Train Loss: {:.6f}'.format(train_acc, train_loss))\n",
    "        print('  Val Acc: {:.6f}   Val Loss: {:.6f}'.format(val_acc, val_loss))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "W5pW9zcKAN-2"
   },
   "source": [
    "#### Visualize accuracy and loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "id": "4ifzgfp7iq2m"
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 864x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.figure(figsize=(12, 4))\n",
    "plt.plot(range(len(train_loss_list)), train_loss_list)\n",
    "plt.plot(range(len(val_loss_list)), val_loss_list, c='r')\n",
    "plt.legend(['train', 'val'])\n",
    "plt.title('Loss')\n",
    "# plt.savefig('loss.png')÷\n",
    "plt.plot(range(len(train_acc_list)), train_acc_list)\n",
    "plt.plot(range(len(val_acc_list)), val_acc_list, c='r')\n",
    "plt.legend(['train', 'val'])\n",
    "plt.title('Acc')\n",
    "plt.savefig('acc_loss.png')\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zMYSgO5viYOk"
   },
   "source": [
    "### Predict Result\n",
    "\n",
    "預測`test`並將結果上傳至Kaggle。[**連結**](https://www.kaggle.com/t/a16786b7da97419f9ba90b495dab08aa)\n",
    "\n",
    "執行完畢此區的程式碼後，會將`test`預測完的結果存下來。\n",
    "\n",
    "上傳流程\n",
    "1. 點選左側選單最下方的資料夾圖示\n",
    "2. 右鍵「result.csv」\n",
    "3. 點選「Download」\n",
    "4. 至連結網頁點選「Submit Predictions」\n",
    "5. 將剛剛下載的檔案上傳\n",
    "6. 系統會計算並公布其中70%資料的正確率"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# model = torch.load('res_e125_val_loss_0.43.pt')\n",
    "ckpt = torch.load('res_acc_e191_val_acc_0.88.pt')\n",
    "model.load_state_dict(ckpt) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "id": "SiyK25P6KXrn"
   },
   "outputs": [],
   "source": [
    "def predict(input_data, model):\n",
    "    model.eval()\n",
    "    output_list = []\n",
    "    with torch.no_grad():\n",
    "        for data in input_data:\n",
    "            images = data.cuda()\n",
    "            outputs = model(images)\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            output_list.extend(predicted.to('cpu').numpy().tolist())\n",
    "    return output_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "id": "0I0LN7HwpnsX"
   },
   "outputs": [],
   "source": [
    "idx = 0\n",
    "output_csv = predict(test_loader, model)\n",
    "with open('result.csv', 'w', newline='') as csvFile:\n",
    "    writer = csv.DictWriter(csvFile, fieldnames=['Id', 'Category'])\n",
    "    writer.writeheader()\n",
    "    for result in output_csv:\n",
    "        idx+=1\n",
    "        writer.writerow({'Id':idx, 'Category':result})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4XTB6EF6mH2Q"
   },
   "source": [
    "# Keep trying and write report\n",
    "\n",
    "持續調整模型、訓練方法、損失函數、優化器等，來訓練出更好的模型，並記錄使用不同參數得出的效果，以利後續 Report 的撰寫。\n",
    "\n",
    "大家加油！"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "Assignment_3_flower_classification.ipynb",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
